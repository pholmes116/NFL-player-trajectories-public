{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c3e8665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e14a40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visible GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        print(f\"✅ Using GPU: {gpus[0].name}\")\n",
    "        # Set mixed precision policy\n",
    "        tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    except RuntimeError as e:\n",
    "        print(\"Failed to set GPU memory growth:\", e)\n",
    "else:\n",
    "    print(\"No GPU found. Using CPU.\")\n",
    "\n",
    "# Set logging\n",
    "tf.debugging.set_log_device_placement(False) # Set it to True to make sure the GPU is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "173d05eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset element specification: (TensorSpec(shape=(4,), dtype=tf.int32, name=None), TensorSpec(shape=(100, 46), dtype=tf.float32, name=None), TensorSpec(shape=(46,), dtype=tf.float32, name=None))\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"../processed_data/transformer_dataset_9\"\n",
    "\n",
    "# Load dataset without any transformations\n",
    "raw_ds = tf.data.Dataset.load(dataset_path)\n",
    "\n",
    "# Print dataset structure\n",
    "print(\"Dataset element specification:\", raw_ds.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "117a222c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cardinality: 2547197\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset cardinality:\",\n",
    "      tf.data.experimental.cardinality(raw_ds).numpy())   # should now print a number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f609c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_split(split_num):\n",
    "    def _filter(meta, x, y):\n",
    "        return tf.equal(meta[2], split_num)\n",
    "    return _filter\n",
    "\n",
    "def drop_meta(meta, x, y):\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6089c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 02:00:35.595410: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:23: Filling up shuffle buffer (this may take a while): 1519 of 4096\n",
      "2025-05-01 02:00:45.614771: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:23: Filling up shuffle buffer (this may take a while): 2385 of 4096\n",
      "2025-05-01 02:01:05.596084: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:23: Filling up shuffle buffer (this may take a while): 3842 of 4096\n",
      "2025-05-01 02:01:08.398560: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:482] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch shape: (2048, 100, 46)\n",
      "y_batch shape: (2048, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 02:01:53.532377: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch shape: (2048, 100, 46)\n",
      "y_batch shape: (2048, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 02:02:30.087571: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch shape: (2048, 100, 46)\n",
      "y_batch shape: (2048, 46)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_ds = (raw_ds\n",
    "            .filter(filter_split(0))\n",
    "            .map(drop_meta, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .shuffle(4096)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "val_ds   = (raw_ds\n",
    "            .filter(filter_split(1))\n",
    "            .map(drop_meta, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "test_ds  = (raw_ds\n",
    "            .filter(filter_split(2))\n",
    "            .map(drop_meta, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# Take one batch from the dataset\n",
    "for x_batch, y_batch in train_ds.take(1):\n",
    "    print(\"x_batch shape:\", x_batch.shape)\n",
    "    print(\"y_batch shape:\", y_batch.shape)\n",
    "\n",
    "for x_batch, y_batch in val_ds.take(1):\n",
    "    print(\"x_batch shape:\", x_batch.shape)\n",
    "    print(\"y_batch shape:\", y_batch.shape)\n",
    "\n",
    "for x_batch, y_batch in test_ds.take(1):\n",
    "    print(\"x_batch shape:\", x_batch.shape)\n",
    "    print(\"y_batch shape:\", y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b50d2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "NUM_FEATS = 46          # x,y for 23 entities\n",
    "MAX_LEN  = 100          # same value you used in dataset builder\n",
    "D_MODEL  = 128          # transformer hidden size\n",
    "N_HEADS  = 4\n",
    "N_LAYERS = 4\n",
    "D_FF     = 512\n",
    "DROPOUT  = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a069e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔═══════════════════╗\n",
    "# ║ 2. Positional enc ║  (learnable 1‑D embedding)\n",
    "# ╚═══════════════════╝\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_emb = self.add_weight(\n",
    "            name=\"pos_emb\",\n",
    "            shape=(max_len, d_model),\n",
    "            initializer=\"uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pos_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1372b6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔═══════════════════════════╗\n",
    "# ║ 3. Padding‑mask function  ║\n",
    "# ╚═══════════════════════════╝\n",
    "class PaddingMask(layers.Layer):\n",
    "    def call(self, x):\n",
    "        # x:  (B, T, F) — zero‐padded on the left\n",
    "        pad = tf.reduce_all(tf.equal(x, 0.0), axis=-1)      # → (B, T)\n",
    "        # reshape to (B, 1, 1, T) for MultiHeadAttention\n",
    "        return pad[:, tf.newaxis, tf.newaxis, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e0ec51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════╗\n",
    "# ║ 4. Transformer encoder ║\n",
    "# ╚════════════════════════╝\n",
    "def transformer_block(d_model, n_heads, d_ff, dropout):\n",
    "    inputs   = layers.Input(shape=(None, d_model))\n",
    "    padding  = layers.Input(shape=(1,1,None), dtype=tf.bool)  # mask\n",
    "\n",
    "    x = layers.MultiHeadAttention(\n",
    "        num_heads=n_heads, key_dim=d_model//n_heads, dropout=dropout\n",
    "    )(inputs, inputs, attention_mask=padding)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs + x)\n",
    "\n",
    "    y = layers.Dense(d_ff, activation=\"relu\")(x)\n",
    "    y = layers.Dense(d_model)(y)\n",
    "    y = layers.Dropout(dropout)(y)\n",
    "    y = layers.LayerNormalization(epsilon=1e-6)(x + y)\n",
    "\n",
    "    return keras.Model([inputs, padding], y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22063c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"NFL_Frame_Predictor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"NFL_Frame_Predictor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ sequence            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cast (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,016</span> │ cast[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cast_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_encoding │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,800</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncodin…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ padding_mask        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cast_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PaddingMask</span>)       │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │ positional_encod… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │ functional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │ functional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_3        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │ functional_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ functional_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pred_xy (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,934</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ sequence            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m46\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cast (\u001b[38;5;33mCast\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m46\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ sequence[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │      \u001b[38;5;34m6,016\u001b[0m │ cast[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cast_1 (\u001b[38;5;33mCast\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m46\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ sequence[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_encoding │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m12,800\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mPositionalEncodin…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ padding_mask        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m100\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ cast_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mPaddingMask\u001b[0m)       │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │    \u001b[38;5;34m198,272\u001b[0m │ positional_encod… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │    \u001b[38;5;34m198,272\u001b[0m │ functional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │    \u001b[38;5;34m198,272\u001b[0m │ functional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_3        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │    \u001b[38;5;34m198,272\u001b[0m │ functional_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ functional_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pred_xy (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m)        │      \u001b[38;5;34m5,934\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">817,838</span> (3.12 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m817,838\u001b[0m (3.12 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">817,838</span> (3.12 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m817,838\u001b[0m (3.12 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ╔════════════════════════════════╗\n",
    "# ║ 5. End‑to‑end prediction model ║\n",
    "# ╚════════════════════════════════╝\n",
    "def build_model(\n",
    "    num_feats=NUM_FEATS,\n",
    "    max_len=MAX_LEN,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=N_HEADS,\n",
    "    n_layers=N_LAYERS,\n",
    "    d_ff=D_FF,\n",
    "    dropout=DROPOUT,\n",
    "):\n",
    "    seq_in  = layers.Input(shape=(max_len, num_feats), name=\"sequence\")   # (B,T,F)\n",
    "\n",
    "    # Linear projection to d_model\n",
    "    x = layers.Dense(d_model)(seq_in)\n",
    "\n",
    "    # Add learnable positional encodings\n",
    "    x = PositionalEncoding(max_len, d_model)(x)\n",
    "\n",
    "    # Build padding mask once\n",
    "    pad_mask = PaddingMask()(seq_in)\n",
    "\n",
    "    # Stack encoder layers\n",
    "    for _ in range(n_layers):\n",
    "        x = transformer_block(d_model, n_heads, d_ff, dropout)([x, pad_mask])\n",
    "\n",
    "    # We need the hidden state that corresponds to *frame t* (the last row)\n",
    "    # – that is always index -1 thanks to left padding.\n",
    "    h_t = layers.Lambda(lambda t: t[:, -1])(x)          # (B, D)\n",
    "\n",
    "    # Regress the 46 co‑ordinates\n",
    "    out = layers.Dense(num_feats, name=\"pred_xy\")(h_t)\n",
    "\n",
    "    return keras.Model(seq_in, out, name=\"NFL_Frame_Predictor\")\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60621355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 02:03:22.317981: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:23: Filling up shuffle buffer (this may take a while): 2851 of 4096\n",
      "2025-05-01 02:03:24.003989: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:482] Shuffle buffer filled.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746061407.204001   35478 service.cc:152] XLA service 0x7f0350001770 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1746061407.204079   35478 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4080 Laptop GPU, Compute Capability 8.9\n",
      "2025-05-01 02:03:27.500516: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1746061409.644903   35478 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-05-01 02:03:32.202143: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-05-01 02:03:32.992669: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 164 bytes spill stores, 164 bytes spill loads\n",
      "\n",
      "2025-05-01 02:03:33.711063: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 144 bytes spill stores, 144 bytes spill loads\n",
      "\n",
      "2025-05-01 02:03:34.700518: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_29', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "2025-05-01 02:03:35.063520: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 144 bytes spill stores, 144 bytes spill loads\n",
      "\n",
      "2025-05-01 02:03:35.078405: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_29', 160 bytes spill stores, 160 bytes spill loads\n",
      "\n",
      "2025-05-01 02:03:35.895193: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9715', 184 bytes spill stores, 184 bytes spill loads\n",
      "\n",
      "2025-05-01 02:03:35.905362: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9529', 400 bytes spill stores, 400 bytes spill loads\n",
      "\n",
      "2025-05-01 02:03:36.357444: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_29', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-05-01 02:03:36.464475: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9823', 176 bytes spill stores, 176 bytes spill loads\n",
      "\n",
      "I0000 00:00:1746061432.621921   35478 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - loss: 0.3856 - mean_absolute_error: 0.4294"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 02:18:26.713471: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot', 108 bytes spill stores, 108 bytes spill loads\n",
      "\n",
      "2025-05-01 02:18:26.751034: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot', 272 bytes spill stores, 272 bytes spill loads\n",
      "\n",
      "2025-05-01 02:18:26.848015: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot', 188 bytes spill stores, 188 bytes spill loads\n",
      "\n",
      "2025-05-01 02:18:27.089758: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 300 bytes spill stores, 300 bytes spill loads\n",
      "\n",
      "2025-05-01 02:18:27.277299: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2025-05-01 02:18:27.558382: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4', 300 bytes spill stores, 300 bytes spill loads\n",
      "\n",
      "2025-05-01 02:18:27.792619: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2025-05-01 02:18:28.890982: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2025-05-01 02:18:29.222151: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 568 bytes spill stores, 552 bytes spill loads\n",
      "\n",
      "2025-05-01 02:18:29.228815: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 156 bytes spill stores, 156 bytes spill loads\n",
      "\n",
      "2025-05-01 02:18:29.441057: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2025-05-01 02:18:29.458824: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-05-01 02:18:29.549679: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5', 300 bytes spill stores, 300 bytes spill loads\n",
      "\n",
      "2025-05-01 02:18:29.652281: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_948', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4041s\u001b[0m 40s/step - loss: 0.3835 - mean_absolute_error: 0.4280 - val_loss: 0.0128 - val_mean_absolute_error: 0.0869\n",
      "Epoch 2/10000\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3940s\u001b[0m 40s/step - loss: 0.0545 - mean_absolute_error: 0.1849 - val_loss: 0.0077 - val_mean_absolute_error: 0.0682\n",
      "Epoch 3/10000\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4049s\u001b[0m 41s/step - loss: 0.0366 - mean_absolute_error: 0.1518 - val_loss: 0.0056 - val_mean_absolute_error: 0.0577\n",
      "Epoch 4/10000\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3989s\u001b[0m 40s/step - loss: 0.0270 - mean_absolute_error: 0.1304 - val_loss: 0.0052 - val_mean_absolute_error: 0.0551\n",
      "Epoch 5/10000\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3923s\u001b[0m 40s/step - loss: 0.0209 - mean_absolute_error: 0.1145 - val_loss: 0.0046 - val_mean_absolute_error: 0.0519\n",
      "Epoch 6/10000\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3843s\u001b[0m 39s/step - loss: 0.0164 - mean_absolute_error: 0.1014 - val_loss: 0.0041 - val_mean_absolute_error: 0.0493\n",
      "Epoch 7/10000\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0134 - mean_absolute_error: 0.0914"
     ]
    }
   ],
   "source": [
    "# ╔════════════════════╗\n",
    "# ║ 6. Compile & train ║\n",
    "# ╚════════════════════╝\n",
    "\n",
    "# ── 1)  Make sure we have a place to put checkpoints ─────────────────\n",
    "WEIGHT_DIR = Path(\"../weights\")\n",
    "WEIGHT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "ckpt_cb = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=(WEIGHT_DIR /\n",
    "              \"epoch_{epoch:03d}-val{val_loss:.6f}.weights.h5\").as_posix(),\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=False,      # save every epoch → “periodic” archive\n",
    "    save_weights_only=True,    # just the weights, not optimizer state\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# ── 2)  Early-stopping ───────────────────────────────────────────────\n",
    "PATIENCE = 5\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# ── 3)  Compile the model ────────────────────────────────────────────\n",
    "LR = 1e-4\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(LR),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=[keras.metrics.MeanAbsoluteError()],\n",
    ")\n",
    "\n",
    "# ── 4)  Fit – stop early, save weights each epoch ────────────────────\n",
    "EPOCHS = 10_000   # high ceiling; early-stop decides real count\n",
    "STEPS_PER_EPOCH = 1_000\n",
    "VAL_STEPS_PER_EPCH = 100\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_steps=VAL_STEPS_PER_EPCH,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[early_stop, ckpt_cb],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# optional: evaluate on test set after training\n",
    "test_loss, test_mae = model.evaluate(test_ds, verbose=1)\n",
    "print(f\"\\n✅  Test MSE: {test_loss:.5f}   |   Test MAE: {test_mae:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔═══════════════╗\n",
    "# ║ 7. Evaluation ║ -\n",
    "# ╚═══════════════╝\n",
    "# Simple end‑to‑end evaluation on a held‑out batch\n",
    "for X_batch, y_batch in val_ds.take(1):\n",
    "    y_pred = model(X_batch)\n",
    "    mse = tf.reduce_mean(tf.square(y_pred - y_batch))\n",
    "    print(\"Validation MSE (batch):\", mse.numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NFL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
